\documentclass[12pt, twoside]{article}
\usepackage{jmlda}
\usepackage[colorinlistoftodos,bordercolor=orange,backgroundcolor=orange!20,linecolor=orange,textsize=scriptsize]{todonotes}
\usepackage{algorithm}\usepackage{algpseudocode}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\alexanderb}[1]{\todo[inline]{{\textbf{Alexander B.:} \emph{#1}}}}
\newcommand{\nikolay}[1]{\todo[inline]{{\textbf{Nikolay:} \emph{#1}}}}
\newcommand{\alexanderg}[1]{\todo[inline]{{\textbf{Alexander G.:} \emph{#1}}}}
\newcommand{\hdir}{.}

\begin{document}

\title
    [Распределенная оптимизация в условиях Поляка-Лоясиевича] % краткое название; не нужно, если полное название влезает в~колонтитул
    {Распределенная оптимизация в условиях Поляка-Лоясиевича}
\author
    [И.\,О.~Автор] % список авторов (не более трех) для колонтитула; не нужен, если основной список влезает в колонтитул
    {И.\,О.~Автор, И.\,О.~Соавтор, И.\,О.~Фамилия} % основной список авторов, выводимый в оглавление
    [И.\,О.~Автор$^1$, И.\,О.~Соавтор$^2$, И.\,О.~Фамилия$^{1,2}$] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\email
    {author@site.ru; co-author@site.ru;  co-author@site.ru}
\thanks
    {Работа выполнена при
     %частичной
     финансовой поддержке РФФИ, проекты \No\ \No 00-00-00000 и 00-00-00001.}
\organization
    {$^1$Организация, адрес; $^2$Организация, адрес}
\abstract
    {В статье рассматривается новый метод децентрализованного распределенного решения больших систем нелинейных уравнений в условиях Поляка-Лоясиевича. Суть метода состоит в постановке эквивалентной задачи распределенной оптимизации и последующем ее сведении сперва к задаче ограниченной оптимизации, а затем к задаче композитной оптимизации, но уже без ограничений. 
Предложенный метод сравнивается с градиентным спуском, ускоренным градиентным спуском, а также с последовательным и параллельным алгоритмом обратного распространения ошибки при обучении многослойной нейронной сети с нелинейной функцией активации нейрона. 
	
\bigskip
\noindent
\textbf{Ключевые слова}: \emph {большие нелинейные системы; распределенная оптимизация; условия Поляка-Лоясиевича; многослойные нейронные сети}
}



%данные поля заполняются редакцией журнала
\doi{10.21469/22233792}
\receivedRus{01.01.2017}
\receivedEng{January 01, 2017}

\maketitle
\linenumbers

\section{Введение}
Во многих современных прикладных задачах возникают подзадачи, требующие решения больших систем нелинейных уравнений. Примерами таких задач могут послужить задача обучения многослойной нейронной сети или задача об оптимальном распределении мощности между вышками сотовой связи. Возникающие в этих задачах огромные системы уравнений разумно решать распределенно.

Вместо нелинейной системы мы будем рассматривать эквивалентную задачу распределенной оптимизации: $\min\limits_{x\in\mathbb{R}^p}f(x) = \sum\limits_{i=1}^nf_i(x)$. В \cite{Karimi1} описывается метод сведения этой задачи к задаче ограниченной оптимизации, которая решается прямодвойственным градиентным методом. Ограничения в данной задаче обусловлены необходимостью совпадения решений на различных процессорах при распределенных вычислениях.

Мы, в свою очередь, сводим задачу ограниченной оптимизации к задаче композитной оптимизации, убрав жесткие условия на совпадения решений на различных процессорах. И предлагаем решать полученную задачу аналогами метода подобных треугольников или слайдинга \cite{Gasnikov2}.

Наш метод сравнивается с методами градиентного спуска и ускоренного градиентного спуска, описанными в \cite{Karimi2} и \cite{Gasnikov1} и имеющими линейную скорость сходимости в условиях Поляка-Лоясиевича. Также мы сравнили наш метод с последовательным и параллельным вариантами самого распространенного на данный момент алгоритма обратного распространения ошибки для обучения нейронных сетей с нелинейной функцией активации нейрона, предложенными в \cite{Prafulla}.

Сравнение производится в ходе вычислительного эксперимента при обучении нейронных сетей с различным количеством слоев и функцией активации нейрона -- сигмоидой. Обучение производится на классических данных (CIFAR, MNIST, IMAGNET).

\section{Название раздела}
Данный документ демонстрирует оформление статьи,
подаваемой в электронную систему подачи статей \url{http://jmlda.org/papers} для публикации в журнале <<Машинное обучение и анализ данных>>.
Более подробные инструкции по~стилевому файлу \texttt{jmlda.sty} и~использованию издательской системы \LaTeXe\
находятся в~документе \texttt{authors-guide.pdf}.
Работу над статьёй удобно начинать с~правки \TeX-файла данного документа.

Обращаем внимание, что данный документ должен быть сохранен в кодировке~\verb'UTF-8 without BOM'.
Для смены кодировки рекомендуется пользоваться текстовыми редакторами \verb'Sublime Text' или \verb'Notepad++'.

\paragraph{Название параграфа}
Разделы и~параграфы, за исключением списков литературы, нумеруются.

\section{Заключение}
Желательно, чтобы этот раздел был, причём он не~должен дословно повторять аннотацию.
Обычно здесь отмечают, каких результатов удалось добиться, какие проблемы остались открытыми.

%%%% если имеется doi цитируемого источника, необходимо его указать, см. пример в \bibitem{article}
%%%% DOI публикации, зарегистрированной в системе Crossref, можно получить по адресу http://www.crossref.org/guestquery/


%%%% если имеется doi цитируемого источника, необходимо его указать, см. пример в \bibitem{article}
%%%% DOI публикации, зарегистрированной в системе Crossref, можно получить по адресу http://www.crossref.org/guestquery/.

\bibliographystyle{plain}
\bibliography{litr}


\end{document}
