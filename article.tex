\documentclass[12pt, twoside]{article}
\usepackage{jmlda}
\usepackage[colorinlistoftodos,bordercolor=orange,backgroundcolor=orange!20,linecolor=orange,textsize=scriptsize]{todonotes}
\usepackage{algorithm}\usepackage{algpseudocode}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\alexanderb}[1]{\todo[inline]{{\textbf{Alexander B.:} \emph{#1}}}}
\newcommand{\nikolay}[1]{\todo[inline]{{\textbf{Nikolay:} \emph{#1}}}}
\newcommand{\alexanderg}[1]{\todo[inline]{{\textbf{Alexander G.:} \emph{#1}}}}
\newcommand{\hdir}{.}

\begin{document}

\title
    [Распределенная оптимизация в условиях Поляка-Лоясиевича] % краткое название; не нужно, если полное название влезает в~колонтитул
    {Распределенная оптимизация в условиях Поляка-Лоясиевича}
\author
    [И.\,О.~Автор] % список авторов (не более трех) для колонтитула; не нужен, если основной список влезает в колонтитул
    {И.\,О.~Автор, И.\,О.~Соавтор, И.\,О.~Фамилия} % основной список авторов, выводимый в оглавление
    [И.\,О.~Автор$^1$, И.\,О.~Соавтор$^2$, И.\,О.~Фамилия$^{1,2}$] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\email
    {author@site.ru; co-author@site.ru;  co-author@site.ru}
\thanks
    {Работа выполнена при
     %частичной
     финансовой поддержке РФФИ, проекты \No\ \No 00-00-00000 и 00-00-00001.}
\organization
    {$^1$Организация, адрес; $^2$Организация, адрес}
\abstract
    {В статье рассматривается новый метод децентрализованного распределенного решения больших систем нелинейных уравнений в условиях Поляка-Лоясиевича. Суть метода состоит в постановке эквивалентной задачи распределенной оптимизации и последующем ее сведении сперва к задаче ограниченной оптимизации, а затем к задаче композитной оптимизации, но уже без ограничений. 
    %\alexanderb{Нужно про метод больше}
    Предложенный метод сравнивается с градиентным спуском, ускоренным градиентным спуском, а также с последовательным и параллельным алгоритмом обратного распространения ошибки при обучении многослойной нейронной сети с нелинейной функцией активации нейрона. 
	
\bigskip
\noindent
\textbf{Ключевые слова}: \emph {большие нелинейные системы; распределенная оптимизация; условия Поляка-Лоясиевича; многослойные нейронные сети}
}



%данные поля заполняются редакцией журнала
\doi{10.21469/22233792}
\receivedRus{01.01.2017}
\receivedEng{January 01, 2017}

\maketitle
\linenumbers

\section{Введение}
За последние несколько лет наблюдается скачок популярности задач, связанных с анализом больших данных. Это вызванно увеличением количества параметров моделей как в машинном обучении, так и в других областях прикладной математики. Примерами таких задач могут послужить задачи обработки непрерывно поступающих данных с измерительных устройств, аудио и видеоматериалов; изучения потоков сообщений в социальных сетях или метеорологических данных; анализа данных о местонахождении абонентов сетей и оптимальное распределение мощности между вышками сотовой связи. В связи с этим возникла потребность решать эти задачи распределенно. Суть многих из этих задач заключается в решении огрромных сислем нелинейных уравнений. 

В этой статье мы раасматриваем распределенный способ решения системы нелинейных уравнений: 
\begin{equation}
    \label{1}
    \begin{cases}
    f_1(x_1,\ldots,x_n) = 0\\
    f_2(x_1,\ldots,x_n) = 0\\
    \cdots\\
    f_m(x_1,\ldots,x_n) = 0\\
    \end{cases}
    f_i:\mathbb{R}^n \rightarrow \mathbb{R},\ i = 1, \ldots, m
\end{equation}
Эту систему можно переписать в виде эквивалентной задачи оптимизации:
\begin{equation}
    \label{2}
    g(x) := \sum\limits_{i = 1}^m f^2_i(x) \rightarrow \min\limits_{x \in \mathbb{R}^n}
\end{equation}
В \cite{Karimi1} эта задача представляется в децентрализованном виде:
\begin{eqnarray}
    \label{3}
    \min\limits_{x_i \in \mathbb{R}^n} &&\sum\limits_{i = 1}^m f^2_i(x_i) \nonumber\\
    \text{s.t.} &&x_1 = x_2 = \ldots = x_m
\end{eqnarray}

 Таким образом, возникает задача ограниченной оптимизации, которую авторы статьи решают методом прямодвойственного градиентного спуска. Причем, если функция $g$ удовлетворяет условиям Поляка-Лоясиевича c константой $\nu > 0$, то есть:
\begin{equation}
    \label{4}
    \dfrac{1}{2}||\nabla g(x)||^2 \geq \nu(g(x) - g^*),\ \forall x \in \mathbb{R}^n;\ g^* = \min\limits_{x \in \mathbb{R}^n}g(x)
\end{equation}
то метод будет иметь линейную скорость сходимости.

Мы, в свою очередь, сводим задачу ограниченной оптимизации к задаче композитной оптимизации, смягчив жесткие условия на совпадение ${x_i}_{i=1}^m$ в задаче (3). И предлагаем решать полученную задачу аналогами метода подобных треугольников или слайдинга \cite{Gasnikov2}.

Наш метод сравнивается с методами градиентного спуска и ускоренного градиентного спуска, описанными в \cite{Karimi2} и \cite{Gasnikov1}. Также мы сравнили наш метод с последовательным и параллельным вариантами самого распространенного на данный момент алгоритма обратного распространения ошибки для обучения нейронных сетей с нелинейной функцией активации нейрона, предложенными в \cite{Prafulla}. Сравнение производится в ходе вычислительного эксперимента при обучении нейронных сетей с различным количеством слоев и функцией активации нейрона -- сигмоидой. Обучение производится на классических данных (CIFAR, MNIST, IMAGNET).
%\alexanderb{После первого предложения статью можно закрывать: не решают распределенно - зачем тогда что-то писать статью про это. Нужно вывернуться как раз в строну того, что современные задачи (в том числе нелинйеные уравнения) настолько большие, что имеет смысл решать их распределенно и вот тут бы примеров таких задач (где возникают большие ситсемы)}

%Во многих современных прикладных задачах возникают подзадачи, требующие решения больших систем нелинейных уравнений. Примерами таких задач могут послужить задача обучения многослойной нейронной сети или задача об оптимальном распределении мощности между вышками сотовой связи. Возникающие в этих задачах огромные системы уравнений разумно решать распределенно.

%\alexanderb{Этот метод нужно описать, потому что даже опытным оптимизаторам он может быть неизвестен, а тем более читателям из машин лернинга.}
%Вместо нелинейной системы мы будем рассматривать эквивалентную задачу распределенной оптимизации: $\min\limits_{x\in\mathbb{R}^p}f(x) = \sum\limits_{i=1}^nf_i(x)$. В \cite{Karimi1} описывается метод сведения этой задачи к задаче ограниченной оптимизации, которая решается прямодвойственным градиентным методом. Ограничения в данной задаче обусловлены необходимостью совпадения решений на различных процессорах при распределенных вычислениях.

%Мы, в свою очередь, сводим задачу ограниченной оптимизации к задаче композитной оптимизации, убрав жесткие условия на совпадения решений на различных процессорах. И предлагаем решать полученную задачу аналогами метода подобных треугольников или слайдинга \cite{Gasnikov2}.


%\alexanderb{Почему про метод всего 1 строчка в предыдущем абзаце? Это не здорово, хотя бы чуть чуть его нужно описать в чем его суть и как он примерно выглядить}

%Наш метод сравнивается с методами градиентного спуска и ускоренного градиентного спуска, описанными в \cite{Karimi2} и \cite{Gasnikov1} и имеющими линейную скорость сходимости в условиях Поляка-Лоясиевича. Также мы сравнили наш метод с последовательным и параллельным вариантами самого распространенного на данный момент алгоритма обратного распространения ошибки для обучения нейронных сетей с нелинейной функцией активации нейрона, предложенными в \cite{Prafulla}.

%\alexanderb{Можно написать в чем прелесть этих методов, что сейчас является стейт оф зе арт для просто систем и систем с точки зрения сетей (есть подозрение что статья господина Степанова на РУССКОМ далеко не стейт оф зе арт).}


%Сравнение производится в ходе вычислительного эксперимента при обучении нейронных сетей с различным количеством слоев и функцией активации нейрона -- сигмоидой. Обучение производится на классических данных (CIFAR, MNIST, IMAGNET).

\section{Постановка задачи}
Итак, перед нами стоит задача решения большой системы нелинейных уравнений (1), которую мы переписали в виде задачи децентрализованной оптимизации (3). Предполагается, что каждая из функций $f_i(x_i)$ будет минимизироваться на отдельном процессоре. А связи между этими процессорами будут обеспечивать равенство решений $\{x_i\}_{i=1}^m$. Cвязи между процессорами можно представить в виде взвешенного неориентированного графа $G$. И пусть $L$ -- это матрица Кирхгофа этого графа.

Теперь введем обозначения:\\
1) $X = (x_1,\ldots,x_m)$ -- матрица, стобцы которой есть вектора аргументов функций $\{f_i\}_{i=1}^m$\\
2) $F(X) = \sum\limits_{i = 1}^m f^2_i(x_i)$ -- целевая функция задачи (3)\\
3) $W = L\otimes I_n$, где $\otimes$ означает произведение Кронекера

В \cite{Karimi1} так же показано, что задача (3) эквивалентна следующей задаче условной оптимизации:
\begin{eqnarray}
    \label{5}
    \min\limits_{X \in \mathbb{R}^{m\times n}} && F(X)
    \nonumber\\
    \text{s.t.} && W^{1/2}X = 0
\end{eqnarray}

Мы же предлагаем убрать жесткие условия и свести задачу (5) к задаче композитной оптимизации:
\begin{eqnarray}
    \label{6}
    \min\limits_{X \in \mathbb{R}^{m\times n}} && F(X) + R||W^{1/2}X||
\end{eqnarray}
Здесь $R$ -- это некоторая правильно подобранная положительная константа.

Алгоритм решения поставленной задачи основан на методе подобных треугольников, описанном в \cite{Gasnikov2}.

\section{Заключение}
Желательно, чтобы этот раздел был, причём он не~должен дословно повторять аннотацию.
Обычно здесь отмечают, каких результатов удалось добиться, какие проблемы остались открытыми.

%%%% если имеется doi цитируемого источника, необходимо его указать, см. пример в \bibitem{article}
%%%% DOI публикации, зарегистрированной в системе Crossref, можно получить по адресу http://www.crossref.org/guestquery/


%%%% если имеется doi цитируемого источника, необходимо его указать, см. пример в \bibitem{article}
%%%% DOI публикации, зарегистрированной в системе Crossref, можно получить по адресу http://www.crossref.org/guestquery/.

\bibliographystyle{plain}
\bibliography{litr}


\end{document}
